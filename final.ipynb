{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7fe1ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import snowflake.connector\n",
    "import configparser\n",
    "import fnmatch\n",
    "from configparser import ConfigParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9f83faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read configuration from properties file\n",
    "# config = configparser.ConfigParser()\n",
    "# config.read('config.properties')\n",
    "\n",
    "# from configparser import ConfigParser\n",
    "\n",
    "# parser = ConfigParser()\n",
    "# parser.read('SNOWFLAKE_CREDS.cfg')\n",
    "# account = parser.get('my_api','account')\n",
    "# user = parser.get('my_api','user')\n",
    "# password = parser.get('my_api','password')\n",
    "# database = parser.get('my_api','database')\n",
    "# schema = parser.get('my_api','schema')\n",
    "\n",
    "# connection_parameters = {\n",
    "#             'account':account,\n",
    "#             'user':user,\n",
    "#             'password':password,\n",
    "#             'database':database,\n",
    "#             'SCHEMA':schema\n",
    "#         }\n",
    "# from snowflake.snowpark import Session\n",
    "\n",
    "# snow_session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46b56b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def put_file(stage,path):\n",
    "    \n",
    "    \n",
    "    path_to_stage = \"\".join(path.split('/')[:-1])\n",
    "    print(f\"snow_session.file.put(r'{path}', '@{stage}/{path_to_stage}')\")\n",
    "#     snow_session.file.put(rf'{path}', f'@{stage}/{path_to_stage}',auto_compress=False)\n",
    "#     time.sleep(5)\n",
    "    curr.execute(f\"PUT 'file://{path}' '@{stage}/{path_to_stage}'\")\n",
    "    stage_path = stage+'/'+path\n",
    "    return stage_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11b8271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(curs,target_table,file_path,delimiter=',',replace=False):\n",
    "    file_format = f\"CREATE OR REPLACE FILE FORMAT my_csv_format TYPE = 'CSV' FIELD_DELIMITER = '{delimiter}'  PARSE_HEADER = TRUE;\"\n",
    "    print(file_format)\n",
    "    curs.execute(file_format)\n",
    "    statement = ' OR REPLACE TABLE' if replace else \"TABLE IF NOT EXISTS \"\n",
    "    query = f\"\"\"\n",
    "                CREATE {statement} {target_table}\n",
    "                USING TEMPLATE (\n",
    "                       SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*))\n",
    "                       WITHIN GROUP (ORDER BY ORDER_ID)\n",
    "                       FROM TABLE(\n",
    "                           INFER_SCHEMA(\n",
    "                                 LOCATION=>'{file_path}',\n",
    "                                 FILE_FORMAT=>'my_csv_format'\n",
    "                                       )\n",
    "                                )\n",
    "                            );\"\"\"\n",
    "    print(query)\n",
    "    curs.execute(query)\n",
    "    print(f'\\n{target_table.upper()} has been created\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd48f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_csv_file(stage_name,csv_file_path, table_name,skip_header = 1,field_delimiter = ',', target_operation = 'append', *args):\n",
    "   \n",
    "    # Connect to Snowflake using the snowflake_config dictionary\n",
    "    parser = ConfigParser()\n",
    "    parser.read('SNOWFLAKE_CREDS.cfg')\n",
    "    account = parser.get('my_api','account')\n",
    "    user = parser.get('my_api','user')\n",
    "    password = parser.get('my_api','password')\n",
    "    database = parser.get('my_api','database')\n",
    "    schema = parser.get('my_api','schema')\n",
    "    role = parser.get('my_api','role')\n",
    "    warehouse = parser.get('my_api','warehouse')\n",
    "    \n",
    "    conn = snowflake.connector.connect( user = user ,password = password,account = account ,warehouse = warehouse ,\n",
    "                                       database = database,schema = schema,role = role)\n",
    "      \n",
    "    curs = conn.cursor()\n",
    "    \n",
    "    stage_path = put_file(stage = stage_name,path = csv_file_path)\n",
    "    \n",
    "    # append or overwrite\n",
    "    if target_operation.casefold() == 'overwrite':\n",
    "        print(f\"{table_name.upper()} is truncated\")\n",
    "        truncate_query = f\"TRUNCATE TABLE {table_name.upper()}\"\n",
    "        curs.execute(truncate_query)\n",
    "    \n",
    "    create_table(curs,table_name,stage_path,delimiter=field_delimiter,replace=False)\n",
    "    # Copy CSV file data into Snowflake table\n",
    "    copy_query = f\"\"\"\n",
    "                        COPY INTO {table_name.upper()} \n",
    "                        FROM '@{database}.{schema}.{stage_path}'\n",
    "                        FILE_FORMAT = (\n",
    "                                TYPE = CSV   \n",
    "                                SKIP_HEADER = {skip_header}\n",
    "                                FIELD_DELIMITER = '{field_delimiter}')\n",
    "                        on_error = continue;\"\"\"\n",
    "    print(copy_query)\n",
    "    curs.execute(copy_query)\n",
    "\n",
    "    curs.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38e61e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snow_session.file.put(r'inbound/data_85.csv', '@internal_stage/inbound')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'curr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_file_delim\u001b[39m\u001b[38;5;124m'\u001b[39m][ind] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     18\u001b[0m                     delim \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_file_delim\u001b[39m\u001b[38;5;124m'\u001b[39m][ind]\n\u001b[1;32m---> 19\u001b[0m                 \u001b[43mingest_csv_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtgt_table\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfield_delimiter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdelim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#                 print(f\"ingest_csv_file({stage_name},{csv_file_path+file},{df['tgt_table'][ind]})\")\u001b[39;00m\n\u001b[0;32m     21\u001b[0m                 shutil\u001b[38;5;241m.\u001b[39mmove(csv_file_path\u001b[38;5;241m+\u001b[39mfile,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchive/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m, in \u001b[0;36mingest_csv_file\u001b[1;34m(stage_name, csv_file_path, table_name, skip_header, field_delimiter, target_operation, *args)\u001b[0m\n\u001b[0;32m     14\u001b[0m conn \u001b[38;5;241m=\u001b[39m snowflake\u001b[38;5;241m.\u001b[39mconnector\u001b[38;5;241m.\u001b[39mconnect( user \u001b[38;5;241m=\u001b[39m user ,password \u001b[38;5;241m=\u001b[39m password,account \u001b[38;5;241m=\u001b[39m account ,warehouse \u001b[38;5;241m=\u001b[39m warehouse ,\n\u001b[0;32m     15\u001b[0m                                    database \u001b[38;5;241m=\u001b[39m database,schema \u001b[38;5;241m=\u001b[39m schema,role \u001b[38;5;241m=\u001b[39m role)\n\u001b[0;32m     17\u001b[0m curs \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[1;32m---> 19\u001b[0m stage_path \u001b[38;5;241m=\u001b[39m \u001b[43mput_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstage_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# append or overwrite\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_operation\u001b[38;5;241m.\u001b[39mcasefold() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[1;32mIn[14], line 10\u001b[0m, in \u001b[0;36mput_file\u001b[1;34m(stage, path)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnow_session.file.put(r\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_to_stage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#     snow_session.file.put(rf'{path}', f'@{stage}/{path_to_stage}',auto_compress=False)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#     time.sleep(5)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mcurr\u001b[49m\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPUT \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_to_stage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m     stage_path \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mpath\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stage_path\n",
      "\u001b[1;31mNameError\u001b[0m: name 'curr' is not defined"
     ]
    }
   ],
   "source": [
    "# def main(csv_file_path):\n",
    "\n",
    "stage_name = 'internal_stage'\n",
    "csv_file_path = 'inbound/'\n",
    "archive_folder = 'archive/'\n",
    "missing_folder = 'missing/'\n",
    "failed = 'failed/'\n",
    "\n",
    "df = pd.read_csv('mapping/metadata1.csv',header = 0)\n",
    "\n",
    "for file in os.listdir(csv_file_path):\n",
    "    matching = False\n",
    "    for ind in df.index:\n",
    "        if fnmatch.fnmatch(file, df['src_file_prefix'][ind]+'*.csv'):\n",
    "            if df['is_active'][ind] == 'Y':\n",
    "                delim = ','\n",
    "                if df['src_file_delim'][ind] != ',':\n",
    "                    delim = df['src_file_delim'][ind]\n",
    "                ingest_csv_file(stage_name,csv_file_path+file,df['tgt_table'][ind],field_delimiter = delim)\n",
    "#                 print(f\"ingest_csv_file({stage_name},{csv_file_path+file},{df['tgt_table'][ind]})\")\n",
    "                shutil.move(csv_file_path+file,'archive/')\n",
    "\n",
    "            else:\n",
    "                print(f'Skipping CSV file {file} for table {df[\"tgt_table\"][ind]} due to inactive flag.')\n",
    "                # Move the skipped file to the missing mapping folder\n",
    "                shutil.move(csv_file_path + file, failed)\n",
    "                print(f'CSV file \"{file}\" moved to missing mapping folder \"{failed}\".')\n",
    "            matching = True\n",
    "            break\n",
    "    if not matching:\n",
    "            print(f'\\n No file name pattern found for CSV file \"{file}\" . Skipping the file.\\n')\n",
    "            shutil.move(csv_file_path + file, missing_folder)\n",
    "            print(f'\\nCSV file \"{file}\" moved to missing mapping folder {missing_folder}\".\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dde301bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snow_session.file.put(r'inbound/data_85.csv', '@internal_stage/inbound')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'curr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minbound/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(csv_file_path)\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_file_delim\u001b[39m\u001b[38;5;124m'\u001b[39m][ind] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     18\u001b[0m                     delim \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_file_delim\u001b[39m\u001b[38;5;124m'\u001b[39m][ind]\n\u001b[1;32m---> 19\u001b[0m                 \u001b[43mingest_csv_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtgt_table\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfield_delimiter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdelim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#                 print(f\"ingest_csv_file({stage_name},{csv_file_path+file},{df['tgt_table'][ind]})\")\u001b[39;00m\n\u001b[0;32m     21\u001b[0m                 shutil\u001b[38;5;241m.\u001b[39mmove(csv_file_path\u001b[38;5;241m+\u001b[39mfile,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchive/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m, in \u001b[0;36mingest_csv_file\u001b[1;34m(stage_name, csv_file_path, table_name, skip_header, field_delimiter, target_operation, *args)\u001b[0m\n\u001b[0;32m     14\u001b[0m conn \u001b[38;5;241m=\u001b[39m snowflake\u001b[38;5;241m.\u001b[39mconnector\u001b[38;5;241m.\u001b[39mconnect( user \u001b[38;5;241m=\u001b[39m user ,password \u001b[38;5;241m=\u001b[39m password,account \u001b[38;5;241m=\u001b[39m account ,warehouse \u001b[38;5;241m=\u001b[39m warehouse ,\n\u001b[0;32m     15\u001b[0m                                    database \u001b[38;5;241m=\u001b[39m database,schema \u001b[38;5;241m=\u001b[39m schema,role \u001b[38;5;241m=\u001b[39m role)\n\u001b[0;32m     17\u001b[0m curs \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[1;32m---> 19\u001b[0m stage_path \u001b[38;5;241m=\u001b[39m \u001b[43mput_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstage_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# append or overwrite\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_operation\u001b[38;5;241m.\u001b[39mcasefold() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m, in \u001b[0;36mput_file\u001b[1;34m(stage, path)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnow_session.file.put(r\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_to_stage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#     snow_session.file.put(rf'{path}', f'@{stage}/{path_to_stage}',auto_compress=False)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#     time.sleep(5)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mcurr\u001b[49m\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPUT \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_to_stage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m     stage_path \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mpath\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stage_path\n",
      "\u001b[1;31mNameError\u001b[0m: name 'curr' is not defined"
     ]
    }
   ],
   "source": [
    "main(r'inbound/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53a83c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metaframework:\n",
    "    \n",
    "    def __init__(self,path):\n",
    "        self.path = path\n",
    "        parser = ConfigParser()\n",
    "        parser.read(rf'{self.path}\\SNOWFLAKE_CREDS.cfg')\n",
    "        account = parser.get('my_api','account')\n",
    "        user = parser.get('my_api','user')\n",
    "        password = parser.get('my_api','password')\n",
    "        database = parser.get('my_api','database')\n",
    "        schema = parser.get('my_api','schema')\n",
    "        role = parser.get('my_api','role')\n",
    "        warehouse = parser.get('my_api','warehouse')\n",
    "\n",
    "        conn = snowflake.connector.connect( user = user ,password = password,account = account ,warehouse = warehouse ,\n",
    "                                           database = database,schema = schema,role = role)\n",
    "        \n",
    "        conn.cursor().execute('create table asshole (id int);')\n",
    "        \n",
    "    def config(self):\n",
    "        pass\n",
    "        \n",
    "    def create_table(self,curs,target_table,file_path,delimiter=',',replace=False):\n",
    "        file_format = f\"CREATE OR REPLACE FILE FORMAT my_csv_format TYPE = 'CSV' FIELD_DELIMITER = '{delimiter}'  PARSE_HEADER = TRUE;\"\n",
    "        print(file_format)\n",
    "        curs.execute(file_format)\n",
    "        statement = ' OR REPLACE TABLE' if replace else \"TABLE IF NOT EXISTS \"\n",
    "        query = f\"\"\"\n",
    "                    CREATE {statement} {target_table}\n",
    "                    USING TEMPLATE (\n",
    "                           SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*))\n",
    "                           WITHIN GROUP (ORDER BY ORDER_ID)\n",
    "                           FROM TABLE(\n",
    "                               INFER_SCHEMA(\n",
    "                                     LOCATION=>'{file_path}',\n",
    "                                     FILE_FORMAT=>'my_csv_format'\n",
    "                                           )\n",
    "                                    )\n",
    "                                );\"\"\"\n",
    "        print(query)\n",
    "        curs.execute(query)\n",
    "        print(f'\\n{target_table.upper()} has been created\\n')\n",
    "    \n",
    "    def put_file(self,stage,path):\n",
    "    \n",
    "    \n",
    "        path_to_stage = \"\".join(path.split('/')[:-1])\n",
    "        print(f\"snow_session.file.put(r'{path}', '@{stage}/{path_to_stage}')\")\n",
    "    #     snow_session.file.put(rf'{path}', f'@{stage}/{path_to_stage}',auto_compress=False)\n",
    "    #     time.sleep(5)\n",
    "        curr.execute(f\"PUT 'file://{path}' '@{stage}/{path_to_stage}'\")\n",
    "        stage_path = stage+'/'+path\n",
    "        return stage_path\n",
    "        \n",
    "        \n",
    "    def ingest_csv_file(self,stage_name,csv_file_path, table_name,skip_header = 1,field_delimiter = ',', target_operation = 'append', *args):\n",
    "   \n",
    "        # Connect to Snowflake using the snowflake_config dictionary\n",
    "        parser = ConfigParser()\n",
    "        parser.read('SNOWFLAKE_CREDS.cfg')\n",
    "        account = parser.get('my_api','account')\n",
    "        user = parser.get('my_api','user')\n",
    "        password = parser.get('my_api','password')\n",
    "        database = parser.get('my_api','database')\n",
    "        schema = parser.get('my_api','schema')\n",
    "        role = parser.get('my_api','role')\n",
    "        warehouse = parser.get('my_api','warehouse')\n",
    "\n",
    "        conn = snowflake.connector.connect( user = user ,password = password,account = account ,warehouse = warehouse ,\n",
    "                                           database = database,schema = schema,role = role)\n",
    "\n",
    "        curs = conn.cursor()\n",
    "\n",
    "        stage_path = put_file(stage = stage_name,path = csv_file_path)\n",
    "\n",
    "        # append or overwrite\n",
    "        if target_operation.casefold() == 'overwrite':\n",
    "            print(f\"{table_name.upper()} is truncated\")\n",
    "            truncate_query = f\"TRUNCATE TABLE {table_name.upper()}\"\n",
    "            curs.execute(truncate_query)\n",
    "\n",
    "        create_table(curs,table_name,stage_path,delimiter=field_delimiter,replace=False)\n",
    "        # Copy CSV file data into Snowflake table\n",
    "        copy_query = f\"\"\"\n",
    "                            COPY INTO {table_name.upper()} \n",
    "                            FROM '@{database}.{schema}.{stage_path}'\n",
    "                            FILE_FORMAT = (\n",
    "                                    TYPE = CSV   \n",
    "                                    SKIP_HEADER = {skip_header}\n",
    "                                    FIELD_DELIMITER = '{field_delimiter}')\n",
    "                            on_error = continue;\"\"\"\n",
    "        print(copy_query)\n",
    "        curs.execute(copy_query)\n",
    "\n",
    "        curs.close()\n",
    "        conn.close()\n",
    "\n",
    "        \n",
    "    def main(self):\n",
    "        stage_name = 'internal_stage'\n",
    "        csv_file_path = 'inbound/'\n",
    "        archive_folder = 'archive/'\n",
    "        missing_folder = 'missing/'\n",
    "        failed = 'failed/'\n",
    "\n",
    "        df = pd.read_csv('mapping/metadata1.csv',header = 0)\n",
    "\n",
    "        for file in os.listdir(self.csv_file_path):\n",
    "            matching = False\n",
    "            for ind in df.index:\n",
    "                if fnmatch.fnmatch(file, df['src_file_prefix'][ind]+'*.csv'):\n",
    "                    if df['is_active'][ind] == 'Y':\n",
    "                        delim = ','\n",
    "                        if df['src_file_delim'][ind] != ',':\n",
    "                            delim = df['src_file_delim'][ind]\n",
    "                        ingest_csv_file(stage_name,self.csv_file_path+file,df['tgt_table'][ind],field_delimiter = delim)\n",
    "        #                 print(f\"ingest_csv_file({stage_name},{csv_file_path+file},{df['tgt_table'][ind]})\")\n",
    "                        shutil.move(csv_file_path+file,'archive/')\n",
    "\n",
    "                    else:\n",
    "                        print(f'Skipping CSV file {file} for table {df[\"tgt_table\"][ind]} due to inactive flag.')\n",
    "                        # Move the skipped file to the missing mapping folder\n",
    "                        shutil.move(csv_file_path + file, failed)\n",
    "                        print(f'CSV file \"{file}\" moved to missing mapping folder \"{failed}\".')\n",
    "                    matching = True\n",
    "                    break\n",
    "            if not matching:\n",
    "                    print(f'\\n No file name pattern found for CSV file \"{file}\" . Skipping the file.\\n')\n",
    "                    shutil.move(csv_file_path + file, missing_folder)\n",
    "                    print(f'\\nCSV file \"{file}\" moved to missing mapping folder {missing_folder}\".\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7723052d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "002002 (42710): SQL compilation error:\nObject 'ASSHOLE' already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mMetaframework\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msnowpark\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m, in \u001b[0;36mMetaframework.__init__\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     13\u001b[0m warehouse \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_api\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarehouse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m conn \u001b[38;5;241m=\u001b[39m snowflake\u001b[38;5;241m.\u001b[39mconnector\u001b[38;5;241m.\u001b[39mconnect( user \u001b[38;5;241m=\u001b[39m user ,password \u001b[38;5;241m=\u001b[39m password,account \u001b[38;5;241m=\u001b[39m account ,warehouse \u001b[38;5;241m=\u001b[39m warehouse ,\n\u001b[0;32m     16\u001b[0m                                    database \u001b[38;5;241m=\u001b[39m database,schema \u001b[38;5;241m=\u001b[39m schema,role \u001b[38;5;241m=\u001b[39m role)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcursor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcreate table asshole (id int);\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38_env\\lib\\site-packages\\snowflake\\connector\\cursor.py:804\u001b[0m, in \u001b[0;36mSnowflakeCursor.execute\u001b[1;34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, file_stream)\u001b[0m\n\u001b[0;32m    800\u001b[0m     is_integrity_error \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    801\u001b[0m         code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100072\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    802\u001b[0m     )  \u001b[38;5;66;03m# NULL result in a non-nullable column\u001b[39;00m\n\u001b[0;32m    803\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m IntegrityError \u001b[38;5;28;01mif\u001b[39;00m is_integrity_error \u001b[38;5;28;01melse\u001b[39;00m ProgrammingError\n\u001b[1;32m--> 804\u001b[0m     \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38_env\\lib\\site-packages\\snowflake\\connector\\errors.py:276\u001b[0m, in \u001b[0;36mError.errorhandler_wrapper\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merrorhandler_wrapper\u001b[39m(\n\u001b[0;32m    255\u001b[0m     connection: SnowflakeConnection \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m     error_value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m    259\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03m        exception to the first handler in that order.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m     handed_over \u001b[38;5;241m=\u001b[39m \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhand_to_other_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handed_over:\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Error\u001b[38;5;241m.\u001b[39merrorhandler_make_exception(\n\u001b[0;32m    284\u001b[0m             error_class,\n\u001b[0;32m    285\u001b[0m             error_value,\n\u001b[0;32m    286\u001b[0m         )\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38_env\\lib\\site-packages\\snowflake\\connector\\errors.py:331\u001b[0m, in \u001b[0;36mError.hand_to_other_handler\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mappend((error_class, error_value))\n\u001b[1;32m--> 331\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38_env\\lib\\site-packages\\snowflake\\connector\\errors.py:210\u001b[0m, in \u001b[0;36mError.default_errorhandler\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_errorhandler\u001b[39m(\n\u001b[0;32m    194\u001b[0m     connection: SnowflakeConnection,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m     error_value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m    198\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Default error handler that raises an error.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m        A Snowflake error.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(\n\u001b[0;32m    211\u001b[0m         msg\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    212\u001b[0m         errno\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    213\u001b[0m         sqlstate\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlstate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    214\u001b[0m         sfqid\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    215\u001b[0m         done_format_msg\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone_format_msg\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    216\u001b[0m         connection\u001b[38;5;241m=\u001b[39mconnection,\n\u001b[0;32m    217\u001b[0m         cursor\u001b[38;5;241m=\u001b[39mcursor,\n\u001b[0;32m    218\u001b[0m     )\n",
      "\u001b[1;31mProgrammingError\u001b[0m: 002002 (42710): SQL compilation error:\nObject 'ASSHOLE' already exists."
     ]
    }
   ],
   "source": [
    "Metaframework(r\"D:\\snowpark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de89ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7165d767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab00fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5157b68f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5adca312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PutResult(source='sales_2023.csv', target='sales_2023.csv', source_size=5003, target_size=5008, source_compression='NONE', target_compression='NONE', status='UPLOADED', message='')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snow_session.file.put(r'inbound/sales_2023.csv', '@internal_stage/inbound',auto_compress=False,overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eddd8b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_1|col_2|col_3|col_4|col_5|col_6|col_7|col_8|col_9|col_10|col_11|col_12|col_13|col_14|col_15|col_16|col_17|col_18|col_19|col_20|col_21|col_22|col_23|col_24|col_25|col_26|col_27|col_28|col_29|col_30|col_31|"
     ]
    }
   ],
   "source": [
    "for i in range(1,32):\n",
    "    print(f'col_{i}',end = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2bfea4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE OR REPLACE FILE FORMAT my_csv_format TYPE = 'CSV' FIELD_DELIMITER = '|'  PARSE_HEADER = TRUE;\n",
      "\n",
      "                CREATE  OR REPLACE TABLE target_table4\n",
      "                USING TEMPLATE (\n",
      "                       SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*))\n",
      "                       WITHIN GROUP (ORDER BY ORDER_ID)\n",
      "                       FROM TABLE(\n",
      "                           INFER_SCHEMA(\n",
      "                                 LOCATION=>'@internal_stage/inbound/sales_2023.csv',\n",
      "                                 FILE_FORMAT=>'my_csv_format'\n",
      "                                       )\n",
      "                                )\n",
      "                            );\n",
      "TARGET_TABLE4 has been created\n"
     ]
    }
   ],
   "source": [
    "create_table(curs,'target_table4','@internal_stage/inbound/sales_2023.csv',delimiter = '|',replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72218567",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ConfigParser()\n",
    "parser.read('SNOWFLAKE_CREDS.cfg')\n",
    "account = parser.get('my_api','account')\n",
    "user = parser.get('my_api','user')\n",
    "password = parser.get('my_api','password')\n",
    "database = parser.get('my_api','database')\n",
    "schema = parser.get('my_api','schema')\n",
    "role = parser.get('my_api','role')\n",
    "warehouse = parser.get('my_api','warehouse')\n",
    "conn = snowflake.connector.connect( user = user ,password = password,account = account ,warehouse = warehouse ,\n",
    "                                       database = database,schema = schema,role = role)\n",
    "      \n",
    "curs = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b06e8999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE OR REPLACE FILE FORMAT my_csv_format TYPE = 'CSV' FIELD_DELIMITER = ','  PARSE_HEADER = TRUE;\n",
      "\n",
      "                CREATE  OR REPLACE TABLE target_table5\n",
      "                USING TEMPLATE (\n",
      "                       SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*))\n",
      "                       WITHIN GROUP (ORDER BY ORDER_ID)\n",
      "                       FROM TABLE(\n",
      "                           INFER_SCHEMA(\n",
      "                                 LOCATION=>'@internal_stage/inbound/data_85.csv',\n",
      "                                 FILE_FORMAT=>'my_csv_format'\n",
      "                                       )\n",
      "                                )\n",
      "                            );\n",
      "TARGET_TABLE5 has been created\n"
     ]
    }
   ],
   "source": [
    "create_table(curs,'target_table5','@internal_stage/inbound/data_85.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96106dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x27e5f5402b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from configparser import ConfigParser\n",
    "\n",
    "\n",
    "parser = ConfigParser()\n",
    "parser.read('SNOWFLAKE_CREDS.cfg')\n",
    "account = parser.get('my_api','account')\n",
    "user = parser.get('my_api','user')\n",
    "password = parser.get('my_api','password')\n",
    "database = parser.get('my_api','database')\n",
    "schema = parser.get('my_api','schema')\n",
    "role = parser.get('my_api','role')\n",
    "warehouse = parser.get('my_api','warehouse')\n",
    "\n",
    "conn = snowflake.connector.connect( user = user ,password = password,account = account ,warehouse = warehouse ,\n",
    "                                   database = database,schema = schema,role = role)\n",
    "\n",
    "curs = conn.cursor()\n",
    "\n",
    "\n",
    "curs.execute(\"PUT 'file://inbound/sales_2023.csv' '@internal_stage/inbound/sales_2024.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af1d122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
